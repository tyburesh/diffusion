
A possible implementation of split (<) and merge (>) in FOREST syntax...

# imports
import numpy as np
# import datetime
import pycuda.autoinit
import pycuda.driver as drv
import pycuda.gpuarray as gpuarray
import pycuda.curandom as curandom
from pycuda.tools import DeviceData
from pycuda.compiler import SourceModule

# constants
MATRIX_SIZE = 8
BLOCK_SIZE = 2
P_LOCAL = 0.25
P_NON_LOCAL = 0.50
GRID_DIMS = MATRIX_SIZE // BLOCK_SIZE
BLOCK_DIMS = BLOCK_SIZE

kernel_code = STUFF

initialize_grid(MATRIX_SIZE):
	grid_a = np.zeros((MATRIX_SIZE, MATRIX_SIZE)).astype(np.float32)					# initialize empty grid
	grid_a[MATRIX_SIZE / 2][MATRIX_SIZE / 2] = 1										# set seed
	return grid_a
	#data_stack.push(grid_a)															# push onto stack

empty_grid():
	grid_b = np.zeros((MATRIX_SIZE, MATRIX_SIZE)).astype(np.float32)					# initialize empty grid
	return grid_b
	#data_stack.push(grid_b)															# push onto stack

initialize_bmsb_kernel:
	kernel = kernel_code.format(MATRIX_SIZE, P_LOCAL, MATRIX_SIZE, P_NON_LOCAL)			# format kernel code w/ constants
	mod = SourceModule(kernel)															# compile kernel code

# loop to pop everything off of the stack
# generic function
split():
	gpu_grib_b = gpuarray.to_gpu(data_stack.pop())										# transfer grid_a to GPU memory
	gpu_grid_a = gpuarray.to_gpu(data_stack.pop())										# transfer grid_b to GPU memory
	#data_stack.push(gpu_grid_a)														# push back onto stack
	#data_stack.push(gpu_grid_b)														# push back onto stack
	return gpu_grid_a, gpu_grid_b

# generic function
merge():
	gpu_grid_b = data_stack.pop()														# pop grid b off stack
	gpu_grid_a = data_stack.pop()														# pop grid a off stack
	grid_a = gpu_grid_a.get()															# transfer data to CPU memory
	return gpu_grid_a																	# return data

local_diffusion():
	f1 = mod.get_function("local_diffuse")												# local diffusion function
	gpu_grid_b = data_stack.pop()														# pop grid b off stack
	gpu_grid_a = data_stack.pop()														# pop grid a off stack
	randoms = random_floats()															# random values between [0,1)
	f1(gpu_grid_a, gpu_grid_b, randoms)													# call kernel function
	gpu_grid_a, gpu_grid_b = gpu_grid_b, gpu_grid_a										# save results back to grid a
	#data_stack.push(gpu_grid_a)														# push grid a back onto stack
	#data_stack.push(gpu_grid_b)														# puch grid b back onto stack
	return gpu_grid_a, return gpu_grid_b

non_local_diffusion():
	f2 = mod.get_function("non_local_diffuse")											# non local diffusion function
	gpu_grid_b = data_stack.pop()														# pop grid b off stack
	gpu_grid_a = data_stack.pop()														# pop grid a off stack
	randoms = random_floats()															# random value between [0,1)
	x_coords, y_coords = random_coords()												# random grid coordinates
	f2(gpu_grid_a, gpu_grid_b, randoms, x_coords, y_coords)								# call kernel function
	gpu_grid_a, gpu_grid_b = gpu_grid_b, gpu_grid_a										# save results back to grid a
	#data_stack.push(gpu_grid_a)														# push grid a back onto stack
	#data_stack.push(gpu_grid_b)														# push grid b back onto stack
	return gpu_grid_a, return gpu_grid_b

random_floats():
	return curandom.rand((MATRIX_SIZE, MATRIX_SIZE))

random_coords():
	a = ((curandom.rand((MATRIX_SIZE, MATRIX_SIZE))) * MATRIX_SIZE).astype(np.int32)
	b = ((curandom.rand((MATRIX_SIZE, MATRIX_SIZE))) * MATRIX_SIZE).astype(np.int32)
	return a, b

write_grid(fname):
	#date = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
	#file_name = 'output-' + date + '.tif'
	f = open(fname, 'w+')
	f.write(grid_a)
	f.close()

initialize_grid(MATRIX_SIZE) == empty_grid < local_diffusion == distance_diffusion > write_grid("output.tif")

